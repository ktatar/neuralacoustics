{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1SapowXFcfr3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "#VIC put here all the new imports that may be needed\n",
    "\n",
    "# from google.colab import drive\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import h5py\n",
    "import sklearn.metrics\n",
    "import torch.nn as nn\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from utilities3 import *\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "import os, os.path\n",
    "\n",
    "import torch\n",
    "\n",
    "v = torch.__version__\n",
    "print(v[2])\n",
    "# assert(v[2] == '8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHKPa0CaS47c",
    "outputId": "95475562-f2d1-495d-9a09-7e426ab46f45"
   },
   "outputs": [],
   "source": [
    "\n",
    "# mount my google drive to access dataset and save model\n",
    "# drive.mount('/content/drive')\n",
    "# !ls \"/content/drive/My Drive/Colab Notebooks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnAf4NpSdQoA"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hg_DT5rrdSUe"
   },
   "outputs": [],
   "source": [
    "#VIC this is the content of: https://github.com/zongyi-li/fourier_neural_operator/blob/master/utilities3.py\n",
    "# it may need to be udpated\n",
    "\n",
    "#################################################\n",
    "#\n",
    "# Utilities\n",
    "#\n",
    "#################################################\n",
    "# reading data\n",
    "class MatReader(object):\n",
    "    def __init__(self, file_path, to_torch=True, to_cuda=False, to_float=True):\n",
    "        super(MatReader, self).__init__()\n",
    "\n",
    "        self.to_torch = to_torch\n",
    "        self.to_cuda = to_cuda\n",
    "        self.to_float = to_float\n",
    "\n",
    "        self.file_path = file_path\n",
    "\n",
    "        self.data = None\n",
    "        self.old_mat = None\n",
    "        self._load_file()\n",
    "\n",
    "    def _load_file(self):\n",
    "        try:\n",
    "            self.data = scipy.io.loadmat(self.file_path)\n",
    "            self.old_mat = True\n",
    "        except:\n",
    "            self.data = h5py.File(self.file_path)\n",
    "            self.old_mat = False\n",
    "\n",
    "    def load_file(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self._load_file()\n",
    "\n",
    "    def read_field(self, field):\n",
    "        x = self.data[field]\n",
    "\n",
    "        if not self.old_mat:\n",
    "            x = x[()]\n",
    "            x = np.transpose(x, axes=range(len(x.shape) - 1, -1, -1))\n",
    "\n",
    "        if self.to_float:\n",
    "            x = x.astype(np.float32)\n",
    "\n",
    "        if self.to_torch:\n",
    "            x = torch.from_numpy(x)\n",
    "\n",
    "            if self.to_cuda:\n",
    "                x = x.cuda()\n",
    "\n",
    "        return x\n",
    "\n",
    "    def set_cuda(self, to_cuda):\n",
    "        self.to_cuda = to_cuda\n",
    "\n",
    "    def set_torch(self, to_torch):\n",
    "        self.to_torch = to_torch\n",
    "\n",
    "    def set_float(self, to_float):\n",
    "        self.to_float = to_float\n",
    "\n",
    "# normalization, pointwise gaussian\n",
    "class UnitGaussianNormalizer(object):\n",
    "    def __init__(self, x, eps=0.00001):\n",
    "        super(UnitGaussianNormalizer, self).__init__()\n",
    "\n",
    "        # x could be in shape of ntrain*n or ntrain*T*n or ntrain*n*T\n",
    "        self.mean = torch.mean(x, 0)\n",
    "        self.std = torch.std(x, 0)\n",
    "        self.eps = eps\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = (x - self.mean) / (self.std + self.eps)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, sample_idx=None):\n",
    "        if sample_idx is None:\n",
    "            std = self.std + self.eps # n\n",
    "            mean = self.mean\n",
    "        else:\n",
    "            if len(self.mean.shape) == len(sample_idx[0].shape):\n",
    "                std = self.std[sample_idx] + self.eps  # batch*n\n",
    "                mean = self.mean[sample_idx]\n",
    "            if len(self.mean.shape) > len(sample_idx[0].shape):\n",
    "                std = self.std[:,sample_idx]+ self.eps # T*batch*n\n",
    "                mean = self.mean[:,sample_idx]\n",
    "\n",
    "        # x is in shape of batch*n or T*batch*n\n",
    "        x = (x * std) + mean\n",
    "        return x\n",
    "\n",
    "    def cuda(self):\n",
    "        self.mean = self.mean.cuda()\n",
    "        self.std = self.std.cuda()\n",
    "\n",
    "    def cpu(self):\n",
    "        self.mean = self.mean.cpu()\n",
    "        self.std = self.std.cpu()\n",
    "\n",
    "# normalization, Gaussian\n",
    "class GaussianNormalizer(object):\n",
    "    def __init__(self, x, eps=0.00001):\n",
    "        super(GaussianNormalizer, self).__init__()\n",
    "\n",
    "        self.mean = torch.mean(x)\n",
    "        self.std = torch.std(x)\n",
    "        self.eps = eps\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = (x - self.mean) / (self.std + self.eps)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, sample_idx=None):\n",
    "        x = (x * (self.std + self.eps)) + self.mean\n",
    "        return x\n",
    "\n",
    "    def cuda(self):\n",
    "        self.mean = self.mean.cuda()\n",
    "        self.std = self.std.cuda()\n",
    "\n",
    "    def cpu(self):\n",
    "        self.mean = self.mean.cpu()\n",
    "        self.std = self.std.cpu()\n",
    "\n",
    "\n",
    "# normalization, scaling by range\n",
    "class RangeNormalizer(object):\n",
    "    def __init__(self, x, low=0.0, high=1.0):\n",
    "        super(RangeNormalizer, self).__init__()\n",
    "        mymin = torch.min(x, 0)[0].view(-1)\n",
    "        mymax = torch.max(x, 0)[0].view(-1)\n",
    "\n",
    "        self.a = (high - low)/(mymax - mymin)\n",
    "        self.b = -self.a*mymax + high\n",
    "\n",
    "    def encode(self, x):\n",
    "        s = x.size()\n",
    "        x = x.view(s[0], -1)\n",
    "        x = self.a*x + self.b\n",
    "        x = x.view(s)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x):\n",
    "        s = x.size()\n",
    "        x = x.view(s[0], -1)\n",
    "        x = (x - self.b)/self.a\n",
    "        x = x.view(s)\n",
    "        return x\n",
    "\n",
    "#loss function with rel/abs Lp loss\n",
    "class LpLoss(object):\n",
    "    def __init__(self, d=2, p=2, size_average=True, reduction=True):\n",
    "        super(LpLoss, self).__init__()\n",
    "\n",
    "        #Dimension and Lp-norm type are postive\n",
    "        assert d > 0 and p > 0\n",
    "\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def abs(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "\n",
    "        #Assume uniform mesh\n",
    "        h = 1.0 / (x.size()[1] - 1.0)\n",
    "\n",
    "        all_norms = (h**(self.d/self.p))*torch.norm(x.view(num_examples,-1) - y.view(num_examples,-1), self.p, 1)\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return torch.mean(all_norms)\n",
    "            else:\n",
    "                return torch.sum(all_norms)\n",
    "\n",
    "        return all_norms\n",
    "\n",
    "    def rel(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "\n",
    "        diff_norms = torch.norm(x.reshape(num_examples,-1) - y.reshape(num_examples,-1), self.p, 1)\n",
    "        y_norms = torch.norm(y.reshape(num_examples,-1), self.p, 1)\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return torch.mean(diff_norms/y_norms)\n",
    "            else:\n",
    "                return torch.sum(diff_norms/y_norms)\n",
    "\n",
    "        return diff_norms/y_norms\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return self.rel(x, y)\n",
    "\n",
    "# A simple feedforward neural network\n",
    "class DenseNet(torch.nn.Module):\n",
    "    def __init__(self, layers, nonlinearity, out_nonlinearity=None, normalize=False):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        self.n_layers = len(layers) - 1\n",
    "\n",
    "        assert self.n_layers >= 1\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for j in range(self.n_layers):\n",
    "            self.layers.append(nn.Linear(layers[j], layers[j+1]))\n",
    "\n",
    "            if j != self.n_layers - 1:\n",
    "                if normalize:\n",
    "                    self.layers.append(nn.BatchNorm1d(layers[j+1]))\n",
    "\n",
    "                self.layers.append(nonlinearity())\n",
    "\n",
    "        if out_nonlinearity is not None:\n",
    "            self.layers.append(out_nonlinearity())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for _, l in enumerate(self.layers):\n",
    "            x = l(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLgIy7iedqsR"
   },
   "source": [
    "# Model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vzkTeyuZdu6V"
   },
   "outputs": [],
   "source": [
    "#VIC this is the content of: https://github.com/zongyi-li/fourier_neural_operator/blob/master/fourier_2d_time.py\n",
    "# it needs to be udpated!\n",
    "# i made a small modification to the original code, please try to preserve it when updating it\n",
    "# the mod is highlighted by the following text #VIC-mod\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "################################################################\n",
    "# fourier layer\n",
    "################################################################\n",
    "\n",
    "class SpectralConv2d_fast(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d_fast, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "class SimpleBlock2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2, width, t_in):\n",
    "        super(SimpleBlock2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "        input shape: (batchsize, x=64, y=64, c=12)\n",
    "        output: the solution of the next timestep\n",
    "        output shape: (batchsize, x=64, y=64, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        #self.fc0 = nn.Linear(12, self.width)\n",
    "        # input channel is 12: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "        \n",
    "        #VIC-mod t_in is passed as parameter now, so that we can decide the number of input time steps\n",
    "        self.fc0 = nn.Linear(t_in+2, self.width)\n",
    "        # input channel: the solution of the previous t_in timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "\n",
    "\n",
    "        self.conv0 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.bn0 = torch.nn.BatchNorm2d(self.width)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(self.width)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(self.width)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(self.width)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        size_x, size_y = x.shape[1], x.shape[2]\n",
    "\n",
    "        grid = self.get_grid(batchsize, size_x, size_y, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x.view(batchsize, self.width, -1)).view(batchsize, self.width, size_x, size_y)\n",
    "        x = self.bn0(x1 + x2)\n",
    "        x = F.relu(x)\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x.view(batchsize, self.width, -1)).view(batchsize, self.width, size_x, size_y)\n",
    "        x = self.bn1(x1 + x2)\n",
    "        x = F.relu(x)\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x.view(batchsize, self.width, -1)).view(batchsize, self.width, size_x, size_y)\n",
    "        x = self.bn2(x1 + x2)\n",
    "        x = F.relu(x)\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x.view(batchsize, self.width, -1)).view(batchsize, self.width, size_x, size_y)\n",
    "        x = self.bn3(x1 + x2)\n",
    "\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_grid(self, batchsize, size_x, size_y, device):\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)\n",
    "\n",
    "class Net2d(nn.Module):\n",
    "    def __init__(self, modes, width, t_in):\n",
    "        super(Net2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        A wrapper function\n",
    "        \"\"\"\n",
    "\n",
    "        self.conv1 = SimpleBlock2d(modes, modes, width, t_in)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def count_params(self):\n",
    "        c = 0\n",
    "        for p in self.parameters():\n",
    "            c += reduce(operator.mul, list(p.size()))\n",
    "\n",
    "        return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWQMCjxIgp0K"
   },
   "source": [
    "# Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8-E0G_JAgsiz"
   },
   "outputs": [],
   "source": [
    "#VIC leave this as it is\n",
    "\n",
    "def dataset_loader(dataset_name, dataset_path, n, win, stride=1, win_lim=-1) :\n",
    "  # get N, T, w and h from file name\n",
    "  datadetails = dataset_name.split(\"_\")\n",
    "  N = datadetails[2][1:]\n",
    "  N = int(N) # num of dataset entries\n",
    "  T = datadetails[3][1:]\n",
    "  T = int(T) # timesteps of each dataset entry\n",
    "  w = datadetails[4][1:]\n",
    "  w = int(w)\n",
    "  h = int(w)\n",
    "  # all the other parameters are dataset specific!\n",
    "\n",
    "  # to grab only a subset of the timesteps in each data entry\n",
    "  if win_lim != -1 and win_lim < T:\n",
    "    T = win_lim\n",
    "\n",
    "  # check that window size is smaller than number of timesteps per each data entry\n",
    "  assert (T >= win)  \n",
    "\n",
    "  # each entry in the dataset is now split in several trainig points, as big as T_in+T_out\n",
    "  #p_num = T-(win-1) # number of points per each dataset entry  \n",
    "  p_num = int( (T-win)/stride ) +1 # number of points per each dataset entry  \n",
    "  p_tot = N * p_num # all training points in dataset\n",
    "  print('Availble points in dataset: ', p_tot)\n",
    "  print('Points requested: ', n)\n",
    "  assert (p_tot >= n)  \n",
    "\n",
    "  # count number of checkpoints, their size and check for remainder file\n",
    "  dataset_full_path = dataset_path + dataset_name + '/'\n",
    "\n",
    "  files = os.listdir(dataset_full_path)\n",
    "  cp = len(files)\n",
    "  rem = 0\n",
    "\n",
    "  for name in files :\n",
    "    splitname = name.split(\"_\")\n",
    "    if splitname[-2] == 'rem' :\n",
    "      rem = 1\n",
    "      cp = cp-1\n",
    "      break\n",
    "\n",
    "  files = sorted(files) # order checkpoint files\n",
    "  cp_size = files[0].split(\"_\")[-1].split(\".\")[0] # read number of dataset entries in each checkpoint\n",
    "  cp_size = int(cp_size)\n",
    "  #cp_size = N//cp # number of dataset entries in each checkpoint\n",
    "  rem_size = 0 # number of dataset entries in remainder file [if any]\n",
    "  if rem > 0 :\n",
    "    rem_size = N - (cp*cp_size)\n",
    "\n",
    "  #print(N, T, w, h, cp, cp_size, rem, rem_size)\n",
    "\n",
    "  # prepare tensor where to load requested data points\n",
    "  u = torch.zeros(n, h, w, win)\n",
    "  #print(u.shape)\n",
    "\n",
    "  # actual sizes with moving window\n",
    "\n",
    "  cp_size_p = cp_size * p_num # number of points per each check point\n",
    "  rem_size_p = rem_size * p_num # number of points in remainder\n",
    "\n",
    "  # let's load\n",
    "\n",
    "  # check how many files we need to cover n points\n",
    "  full_files = n//(cp_size_p)\n",
    "  extra_datapoints = n%cp_size_p\n",
    "\n",
    "  extra_file_needed = extra_datapoints>0\n",
    "\n",
    "  print('Retrieved over', full_files, 'full files,', cp_size_p, 'points each')\n",
    "\n",
    "  # check that all numbers are fine\n",
    "  assert (full_files+extra_file_needed <= cp+rem)\n",
    "\n",
    "  \n",
    "  #print(files)\n",
    "  \n",
    "\n",
    "  # first load from files we will read completely \n",
    "  cnt = 0\n",
    "  for f in range(0,full_files) :\n",
    "    dataloader = MatReader(dataset_full_path+files[f])\n",
    "    uu = dataloader.read_field('u')\n",
    "    #print(f, files[f])\n",
    "    # unroll all entries with moving window\n",
    "    for e in range(0, cp_size) :\n",
    "      # window extracts p_num points from each dataset entry\n",
    "      #print('e', e, 'p_num', p_num)\n",
    "      for tt in range(0, p_num) :\n",
    "        #print('tt', tt)\n",
    "        t = tt*stride\n",
    "        #print('t', t)\n",
    "        u[cnt:cnt+1,...] = uu[e,:,:,t:t+win]\n",
    "        cnt = cnt+1\n",
    "\n",
    "  #print(cnt, extra_datapoints)\n",
    "  \n",
    "\n",
    "  # then load any possible remainder from a further file\n",
    "  if extra_datapoints>0 :\n",
    "    print('Plus', extra_datapoints, 'points from further file')\n",
    "    extra_entries = (extra_datapoints+0.5)//p_num # ceiling to be sure to have enough entries to unroll\n",
    "    dataloader = MatReader(dataset_full_path+files[full_files])\n",
    "    uu = dataloader.read_field('u')\n",
    "    entry = -1\n",
    "    while cnt < n :\n",
    "      entry = entry+1\n",
    "      for tt in range(0,p_num) :\n",
    "        t = tt*stride\n",
    "        u[cnt:cnt+1,...] = uu[entry,:,:,t:t+win] \n",
    "        cnt = cnt+1\n",
    "        if cnt >= n :\n",
    "          break\n",
    "\n",
    "  return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeHE_R1vd9de"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dNGG72ZeABN",
    "outputId": "257a9988-5e1d-4d29-cc93-d4a4d638593c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.0025 100 0.5\n"
     ]
    }
   ],
   "source": [
    "#VIC leave this as it is\n",
    "# then you will be able to play around with the settings\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# editable simulation parameters\n",
    "ntrain = 15000\n",
    "ntest = 2000\n",
    "\n",
    "modes = 12\n",
    "width = 32\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "epochs = 500\n",
    "learning_rate = 0.0025\n",
    "scheduler_step = 100\n",
    "scheduler_gamma = 0.5\n",
    "\n",
    "print(epochs, learning_rate, scheduler_step, scheduler_gamma)\n",
    "\n",
    "T_in = 10\n",
    "T_out = 10\n",
    "# T_in+T_out is window size!\n",
    "\n",
    "win_stride = 1\n",
    "win_lim = -1 #(T_in+T_out)*200 #-1 for no limit\n",
    "\n",
    "# dataset\n",
    "dataset_name = 'iwe_d0_n1000_t50_s64_mu0@1_rho0@5_gamma1'\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "MODEL_ID = '2d_win'\n",
    "\n",
    "dataset_path = '/home/sistla.a/FNO/'\n",
    "\n",
    "# retrieve dataset details and check them\n",
    "splitname = dataset_name.split('_')\n",
    "\n",
    "DATASET = splitname[1]\n",
    "\n",
    "S = splitname[4]\n",
    "S = int(S[1:])\n",
    "\n",
    "mu = splitname[5][2:]\n",
    "rho = splitname[6][3:]\n",
    "gamma = splitname[7][5:]\n",
    "\n",
    "# prepare to save model\n",
    "model_name = 'iwe_m'+MODEL_ID+'_'+DATASET+'_n'+str(ntrain)+'+'+str(ntest)+'_e'+str(epochs)+'_m'+str(modes)+'_w'+ str(width)+'_ti'+str(T_in)+'_to'+str(T_out)+'_ws'+str(win_stride)+'_wl'+str(win_lim)+'_s'+str(S)+'_m'+mu+'_r'+rho+'_g'+gamma\n",
    "\n",
    "model_path = dataset_path[:-9]+'/models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPUr0mA2d3TA"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8htbNF3ro6Ky",
    "outputId": "a9e728d4-7670-4752-d6b2-0ab0d3dd9879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Availble points in dataset:  31000\n",
      "Points requested:  17000\n",
      "Retrieved over 3 full files, 4960 points each\n",
      "Plus 2120 points from further file\n",
      "torch.Size([15000, 64, 64, 10]) torch.Size([2000, 64, 64, 10])\n",
      "preprocessing finished, time used: 3.76432894699974 s\n",
      "train input shape: torch.Size([15000, 64, 64, 10])  output shape:  torch.Size([15000, 64, 64, 10])\n"
     ]
    }
   ],
   "source": [
    "#VIC i think this needs only a minor update, i.e., removing the padding of the location\n",
    "\n",
    "t1 = default_timer()\n",
    "\n",
    "u = dataset_loader(dataset_name, dataset_path, ntrain+ntest, T_in+T_out, win_stride, win_lim)\n",
    "\n",
    "train_a = u[:ntrain,:,:,:T_in]\n",
    "train_u = u[:ntrain,:,:,T_in:T_in+T_out]\n",
    "\n",
    "ntest_start = ntrain\n",
    "test_a = u[ntest_start:ntest_start+ntest,:,:,:T_in]\n",
    "test_u = u[ntest_start:ntest_start+ntest:,:,:,T_in:T_in+T_out]\n",
    "\n",
    "#test_a = u[-ntest:,:,:,:T_in]\n",
    "#test_u = u[-ntest:,:,:,T_in:T_in+T_out]\n",
    "\n",
    "\n",
    "print(train_u.shape, test_u.shape)\n",
    "assert (S == train_u.shape[-2])\n",
    "assert (T_out == train_u.shape[-1])\n",
    "\n",
    "train_a = train_a.reshape(ntrain,S,S,T_in)\n",
    "test_a = test_a.reshape(ntest,S,S,T_in)\n",
    "\n",
    "#VIC should be removed\n",
    "# pad the location (x,y)\n",
    "# gridx = torch.tensor(np.linspace(0, 1, S), dtype=torch.float)\n",
    "# gridx = gridx.reshape(1, S, 1, 1).repeat([1, 1, S, 1])\n",
    "# gridy = torch.tensor(np.linspace(0, 1, S), dtype=torch.float)\n",
    "# gridy = gridy.reshape(1, 1, S, 1).repeat([1, S, 1, 1])\n",
    "\n",
    "# train_a = torch.cat((gridx.repeat([ntrain,1,1,1]), gridy.repeat([ntrain,1,1,1]), train_a), dim=-1)\n",
    "# test_a = torch.cat((gridx.repeat([ntest,1,1,1]), gridy.repeat([ntest,1,1,1]), test_a), dim=-1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_a, train_u), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_a, test_u), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "t2 = default_timer()\n",
    "\n",
    "print('preprocessing finished, time used:', t2-t1, 's')\n",
    "print('train input shape:',train_a.shape, ' output shape: ', train_u.shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y46kBR84qIsa"
   },
   "source": [
    "# Build and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1x6rURfQqLCf",
    "outputId": "6aaf3320-3339-4489-ba2f-f581306c0861",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Utilizing CUDA\n",
      "0\n",
      "NVIDIA Tesla V100-SXM2-32GB\n",
      "True\n",
      "1188897\n",
      "0 92.66330848299549 0.44901635116577154 0.4463325122833252 0.5619339340209961 0.5519152538776397\n",
      "1 80.68466146901483 0.36385437792460124 0.3605355248769124 0.4789548271179199 0.47146665382385256\n",
      "2 77.36002092200215 0.32260152926127117 0.31952800219853716 0.4194711044311523 0.414217883348465\n",
      "3 77.43335708600353 0.29513785484313965 0.2915635955174764 0.3781543382644653 0.3724345338344574\n",
      "4 78.01249934599036 0.28278313227335616 0.27840461196899413 0.3777116474151611 0.37113689255714416\n",
      "5 90.26177638900117 0.270436529006958 0.26555169226328534 0.35770223293304443 0.3502245526313782\n",
      "6 79.63279664699803 0.26377610773722326 0.2586038496017456 0.33302760753631594 0.32641257739067076\n",
      "7 84.42480378900655 0.25707204630533853 0.2516940808614095 0.32712721767425534 0.31991060709953306\n",
      "8 77.23714270000346 0.25194343020121257 0.24643474116325378 0.31894532909393314 0.3118419418334961\n",
      "9 77.44515137100825 0.24840573562622073 0.242776561832428 0.3159011747360229 0.30846447706222535\n",
      "10 77.30521483399207 0.24435768178304035 0.23863077987035117 0.3063573644638061 0.2989329752922058\n",
      "11 86.0544696120196 0.24073690849304202 0.23493092856407166 0.2981649078369141 0.2907973040342331\n",
      "12 83.00705065700458 0.23657918746948242 0.2305713783899943 0.28837820863723757 0.2808433963060379\n",
      "13 77.41093294302118 0.23349303268432617 0.22734419782956442 0.28759472618103027 0.27992014622688294\n",
      "14 77.39580927998759 0.23038762682596842 0.22410541175206503 0.28502477769851686 0.2768225566148758\n",
      "15 86.46112144499784 0.2272371080271403 0.22085310117403667 0.2815722682952881 0.27358016896247866\n",
      "16 91.18720428002416 0.22492836807250977 0.218413525390625 0.30006110725402835 0.2916947785615921\n",
      "17 99.94883777000359 0.22392131655375164 0.21739797848065695 0.2711024430274963 0.26297369730472564\n",
      "18 79.26252165800543 0.22111069376627604 0.2145225818157196 0.27648089237213136 0.2680924564599991\n",
      "19 77.2628703720111 0.22050035029093426 0.21389128023783366 0.27151311311721804 0.2635176389217377\n",
      "20 77.33722056998522 0.2169026810582479 0.21028298559188843 0.27956227025985714 0.27099025118350983\n",
      "21 78.6528261420026 0.21677169057210288 0.2101272109190623 0.27073094348907467 0.262311598777771\n",
      "22 77.21505590999732 0.2154797903315226 0.20879227873484293 0.2730250787734986 0.2644570555686951\n",
      "23 77.2946092679922 0.21452864086151124 0.20782078927357991 0.2606503067970276 0.25237151396274565\n",
      "24 77.23641442300868 0.2140618401590983 0.20731798969904583 0.2589678317070007 0.2506035339832306\n",
      "25 77.51969998900313 0.21198577266693114 0.20526541339556376 0.26930885601043697 0.2610294660329819\n",
      "26 78.41479067798355 0.21173966405232747 0.20505259188016256 0.2607768563270569 0.2525389870405197\n",
      "27 77.1502941630024 0.21048576840718586 0.20372688398361205 0.26397266578674317 0.2553345606327057\n",
      "28 79.24976839701412 0.21006864527384442 0.20328479932149252 0.2564888807296753 0.24807605147361755\n",
      "29 79.21296311399783 0.20982538361867267 0.20300813897450765 0.27121613206863404 0.2622477562427521\n",
      "30 78.66516813301132 0.20964293248494464 0.20284391956329345 0.2642157941818237 0.25564847123622897\n",
      "31 77.13101891300175 0.20801801615397136 0.20121292022069295 0.2666939567565918 0.2577921612262726\n",
      "32 77.18715200500446 0.20574567315419517 0.198948433192571 0.2607208448410034 0.25206841707229616\n",
      "33 77.13194823698723 0.2061560616048177 0.19933114376068115 0.2576342491149902 0.24875573229789733\n",
      "34 79.31715343001997 0.20484219992319744 0.1980259072303772 0.25742603588104246 0.24888470256328582\n",
      "35 80.82958588300971 0.20458371620178223 0.1977582664648692 0.2512669630050659 0.24282290649414062\n",
      "36 78.56085054800496 0.20316048882802326 0.19636862085660298 0.2592012063026428 0.25039414823055267\n",
      "37 77.25576328800526 0.2030205896250407 0.196200857035319 0.2460166399002075 0.23760610938072205\n",
      "38 78.78057882699068 0.20418748134613035 0.197322132174174 0.2536029640197754 0.24512363314628602\n",
      "39 77.37846515298588 0.20043313280741373 0.19366383763949077 0.25330249223709106 0.2446805523633957\n",
      "40 84.00809224299155 0.2017252115503947 0.19489892916679383 0.2557899905204773 0.24721236681938172\n",
      "41 92.37584604602307 0.20103264593760173 0.19424801890055338 0.25319964962005614 0.2444671564102173\n",
      "42 79.39555581699824 0.20129091445922853 0.19446616700490316 0.25032143898010256 0.24172866785526276\n",
      "43 83.29872632498154 0.19902498254140216 0.19226542139053346 0.2466221338272095 0.23827087688446044\n",
      "44 78.22329795898986 0.19840860702514648 0.19165968356132507 0.24518271083831786 0.2366309415102005\n",
      "45 81.23419435901451 0.19931244327545167 0.1925242142836253 0.24780308160781858 0.23944084453582765\n",
      "46 78.25719512399519 0.19889631561279297 0.19213414293924969 0.250178064250946 0.2417496634721756\n",
      "47 80.11044058398693 0.19776218176523847 0.1910239312171936 0.2426772871017456 0.2342871700525284\n",
      "48 77.16107534899493 0.19696033521016437 0.19025513842900593 0.23987955808639527 0.2315947333574295\n",
      "49 81.95231039400096 0.19737553641001385 0.19065418890317282 0.250046325969696 0.24142400753498078\n",
      "50 77.17064153699903 0.19708109415690106 0.19034788778622946 0.24812789936065674 0.23970136511325837\n",
      "51 80.33194958398235 0.19693087568918863 0.190205637439092 0.24619324169158935 0.23767163109779357\n",
      "52 77.16526286501903 0.19603255418141682 0.18931868844032287 0.2431143094062805 0.2347527072429657\n",
      "53 77.14542585701565 0.19654650499979656 0.18981352550188701 0.2442473994255066 0.23584205818176268\n",
      "54 77.21841737101204 0.19642305108388264 0.18959887776374817 0.250869815158844 0.2421044125556946\n",
      "55 77.38733024598332 0.19160165512084962 0.18471275992393493 0.2370220986366272 0.228516793012619\n",
      "56 77.17407459102105 0.19241563921610513 0.1854415248711904 0.2439667260169983 0.23528682672977447\n",
      "57 77.4435865250125 0.18888373254140217 0.1819878355026245 0.23467367887496948 0.22622201657295227\n",
      "58 85.50758729298832 0.18907718706766766 0.18215943624178568 0.23306626138687134 0.22463501822948456\n",
      "59 81.60077016198193 0.18871620814005535 0.18179498551686604 0.23307229404449462 0.22473107957839966\n",
      "60 77.00538612098899 0.18694332300821942 0.1800383628686269 0.23334566335678103 0.2248479722738266\n",
      "61 77.05628368799808 0.18525541778564453 0.17836684505144754 0.2359300775527954 0.22737726986408233\n",
      "62 77.76010536999092 0.18584248925526936 0.1789231987317403 0.22944043006896972 0.22111402082443238\n",
      "63 76.95860119297868 0.18361945542653402 0.17672696644465127 0.23287135677337645 0.2244167765378952\n",
      "64 77.06265004200395 0.18388443247477212 0.1769747998714447 0.22726513833999634 0.21892610955238342\n",
      "65 78.25243444999796 0.1813678463490804 0.1745248922665914 0.2342380458831787 0.22583894658088685\n",
      "66 76.94045041702338 0.18148450457255044 0.1746326736132304 0.22502052612304685 0.21677793610095977\n",
      "67 76.93223860100261 0.18076351905822755 0.1739250189304352 0.22378839359283448 0.21549669802188873\n",
      "68 77.09334343398223 0.18106220165252687 0.17421133378346762 0.23185345706939695 0.2233817641735077\n",
      "69 77.35308398201596 0.17901847184499103 0.17221273969014486 0.21874899253845217 0.21048530411720276\n",
      "70 78.83947300800355 0.17967276706695556 0.1728404545466105 0.21792431440353394 0.2097229688167572\n",
      "71 86.55142134899506 0.17928841570536297 0.17249176019032797 0.23028335132598876 0.22210524129867554\n",
      "72 77.78238369399332 0.17923819938659669 0.1724044415950775 0.22543683767318728 0.2171685633659363\n",
      "73 79.45396610200987 0.1778467389424642 0.17108402336438497 0.22480968770980833 0.21653843426704406\n",
      "74 76.9333025770029 0.17672039807637532 0.16996076695124307 0.2278790259361267 0.2195817708969116\n",
      "75 77.01612784698955 0.17803125597635905 0.1712456332365672 0.21644393606185913 0.20832204520702363\n",
      "76 77.00909281900385 0.1751240678914388 0.16838268478711446 0.21713797168731688 0.20902279937267304\n",
      "77 77.98407547301031 0.1758484736887614 0.169096711174647 0.22025709085464476 0.21194354844093322\n",
      "78 76.93558942101663 0.1743735991032918 0.1676545946121216 0.21872611045837403 0.21061018645763396\n",
      "79 78.71123777399771 0.17401673931121825 0.16728661653200785 0.21785415391921997 0.2094877245426178\n",
      "80 77.53091656498145 0.17497488587697346 0.16820671884218852 0.21656918230056763 0.20855013823509216\n",
      "81 78.75960115599446 0.17382356297810872 0.1671025596777598 0.21850153465270994 0.21046408820152282\n",
      "82 78.94878229300957 0.1730073382059733 0.1662792713324229 0.2144756236076355 0.20650172448158263\n",
      "83 80.24315097701037 0.17151997742970784 0.1648395970662435 0.21555907459259033 0.20759404063224793\n",
      "84 81.018118115986 0.17227947241465252 0.1656331005414327 0.21746220436096192 0.20933706414699554\n",
      "85 76.97099228500156 0.16937907627105714 0.16278213923772175 0.22522375459671024 0.21709532356262207\n",
      "86 79.00573540100595 0.1702614306004842 0.16363447058995564 0.2203443105697632 0.21233599507808684\n",
      "87 90.10287324999808 0.17158838841756185 0.16491291564305624 0.21934719314575196 0.2112386474609375\n",
      "88 79.75319366101758 0.1695944291559855 0.1629699776649475 0.21836489143371582 0.21033251976966857\n",
      "89 77.36334868500126 0.16949227418263751 0.1628730320930481 0.2154642251968384 0.20752718949317933\n",
      "90 76.99608273297781 0.1692849277369181 0.1626571771621704 0.21304430780410769 0.2050236691236496\n",
      "91 76.97615525999572 0.16961640986124676 0.1629676838239034 0.209881893825531 0.20196596384048462\n",
      "92 77.37587516300846 0.16900558780670166 0.16247171150843304 0.2124720145225525 0.2043510011434555\n",
      "93 77.15815194699098 0.16786609804789226 0.16131396851539612 0.20959406089782714 0.20188499796390533\n",
      "94 76.95769524600473 0.16924931224822998 0.16264117875099182 0.21471456880569456 0.20674619936943053\n",
      "95 81.60356406198116 0.16963349730173746 0.16297457664807638 0.21154450483322146 0.20359510290622712\n",
      "96 77.28776908002328 0.16739091462453207 0.16081383199691773 0.21590178232192994 0.20793230998516082\n",
      "97 76.95808091200888 0.16833609954833983 0.1617189240773519 0.20875991706848143 0.2007623052597046\n",
      "98 78.94224638998276 0.16867818850199381 0.16205964555740357 0.20932800378799438 0.20128727400302887\n",
      "99 86.94233699401957 0.16710111632029218 0.16056262482007344 0.2021342519760132 0.19455338037014008\n",
      "100 77.32922422801494 0.15444339586893718 0.1484148857275645 0.1945113163948059 0.18700089645385742\n",
      "101 77.12378676098888 0.15244735469818116 0.1464932672818502 0.19365673685073853 0.18620984590053558\n",
      "102 76.96897014300339 0.15303006331125896 0.1470340058962504 0.19234099493026732 0.18484017503261566\n",
      "103 80.70920571897295 0.1520629794184367 0.14609912371635436 0.18825723028182983 0.18093518328666686\n",
      "104 82.44135095298407 0.15204160966237384 0.14607679875691731 0.19249753274917603 0.18510309243202208\n",
      "105 77.16859245699015 0.15149173344930014 0.14554979707400004 0.1866801399230957 0.1793900545835495\n",
      "106 77.42437043899554 0.14993374111175536 0.14405577958424887 0.18742007675170896 0.18014024126529693\n",
      "107 81.32110502198339 0.15068192024230959 0.14475657458305358 0.1876177186012268 0.1803032635450363\n",
      "108 78.83976572600659 0.15038972981770832 0.1444792157649994 0.18699439430236817 0.17973582965135573\n",
      "109 78.05659853198449 0.1500092857869466 0.14410885566075643 0.1913507565498352 0.18384989720582962\n",
      "110 78.59762546300772 0.14994699118296306 0.14405247995058695 0.18725658864974976 0.1798241553902626\n",
      "111 77.07563682598993 0.15068757685343423 0.14475015851656595 0.18881582946777345 0.18155051463842392\n",
      "112 76.94724941402092 0.1497409392038981 0.1438492655436198 0.18444522914886474 0.17717442119121551\n",
      "113 78.89947924300213 0.14986318663279213 0.14396528202692668 0.19493735485076905 0.1875986530780792\n",
      "114 82.0500274829974 0.1497966470972697 0.14389686350822448 0.18711412057876586 0.17988562905788422\n",
      "115 76.96992566500558 0.14983238914489747 0.14392804096539816 0.18680716590881347 0.17950628221035003\n",
      "116 79.18840855901362 0.14937374172210693 0.14352080637613931 0.19235686588287354 0.18560741138458253\n",
      "117 79.59323194497847 0.14939122385660808 0.14349596263567607 0.19124481315612793 0.1841883054971695\n",
      "118 81.25976402699598 0.14856514012654623 0.14271077132225038 0.1855797248840332 0.17834409093856812\n",
      "119 98.47365181398345 0.14888326805114746 0.14300729014078775 0.1837713539123535 0.1765453816652298\n",
      "120 100.09201632099575 0.1489743016688029 0.14309718635082244 0.18598209133148194 0.17870077574253082\n",
      "121 77.97651330198278 0.14801211528778074 0.1421733412583669 0.18630858182907103 0.1791799782514572\n",
      "122 76.92777995401411 0.14792800225575764 0.1420936891555786 0.18270540647506714 0.17555370527505876\n",
      "123 76.9306905339763 0.1492349301401774 0.1433498830318451 0.18272497539520263 0.17562009185552596\n",
      "124 76.94293999500223 0.1479692793273926 0.14213721075057983 0.182703111743927 0.17540162658691405\n",
      "125 78.18255869997665 0.147394449857076 0.14157975217501323 0.18421192359924315 0.17694331234693528\n",
      "126 77.03155714299646 0.1484531366856893 0.14259578170776366 0.1847978274345398 0.1776718255877495\n",
      "127 76.98676623901702 0.14854797523498536 0.14269147097269694 0.19777902641296388 0.19050703155994414\n",
      "128 77.02975025901105 0.14813866560618083 0.1422878247578939 0.18510565214157104 0.17782820349931716\n",
      "129 82.69293324102182 0.14731490904490152 0.141496213833491 0.18398267793655396 0.17676838529109956\n",
      "130 93.64727869001217 0.1472631843185425 0.14144437937736512 0.1843113597869873 0.1771008120775223\n",
      "131 98.78007386400714 0.14785299051920572 0.14201030972003936 0.18451087312698364 0.1773389186859131\n",
      "132 83.5615889710025 0.1469554026031494 0.1411614966948827 0.18520723981857298 0.17799350428581237\n",
      "133 77.05986747902352 0.14764030038197834 0.14180298659006754 0.18428966245651246 0.17710466599464417\n",
      "134 79.28907374999835 0.1467721293004354 0.14098412850697836 0.18169296607971192 0.17445696169137956\n",
      "135 79.59790140201221 0.14660995876312255 0.14082458357016245 0.18203058214187623 0.1749182721376419\n",
      "136 89.79574643299566 0.14633445584615073 0.14055730736255645 0.18878949346542356 0.18157728976011275\n",
      "137 93.0309158260061 0.14666971900939943 0.14087178155581156 0.1817955369949341 0.17479575562477112\n",
      "138 77.00103249799577 0.14691544808705648 0.1411092520236969 0.17980459880828858 0.17280218613147735\n",
      "139 79.11741715000244 0.14658242530822754 0.14078432235717772 0.1852643976211548 0.17802740061283112\n",
      "140 77.13319062700612 0.14602014736175536 0.14025262605349223 0.1844882453918457 0.17733460432291032\n",
      "141 76.98471527997754 0.14563176047007242 0.13988570760091146 0.18552301511764527 0.17827799320220947\n",
      "142 77.09894830500707 0.14702374722798667 0.1412001890818278 0.18111996326446533 0.17400576668977738\n",
      "143 77.40869454000494 0.1461427547200521 0.1403717179854711 0.18242952642440796 0.17539694422483446\n",
      "144 77.04066734900698 0.1455601268641154 0.13981189516385395 0.18630220012664794 0.1792704795598984\n",
      "145 77.05238584399922 0.14720236384073893 0.14137797582149506 0.18059289360046388 0.17358958613872527\n",
      "146 76.99524680600734 0.14505224338531494 0.13932154161135354 0.1810302968978882 0.17395702159404755\n",
      "147 77.37284802200156 0.14489628476460775 0.13917083058357238 0.18135919580459595 0.1742726936340332\n",
      "148 76.97027536100359 0.14512694852193198 0.1393920197168986 0.18327094373703004 0.17608571064472198\n",
      "149 76.98325159700471 0.14594659339904786 0.14016730666160584 0.18424007892608643 0.17696154302358627\n",
      "150 77.91583456200897 0.144569115524292 0.13885135457515715 0.17994929780960084 0.17302473306655883\n",
      "151 77.13383883799543 0.1453025002924601 0.13955699412822722 0.17909150972366333 0.17211103308200837\n",
      "152 77.00550305697834 0.14480691073099772 0.1390876999060313 0.1844903606414795 0.17736987447738647\n",
      "153 78.82242731901351 0.14564214452107746 0.13989803268114726 0.18416509466171266 0.1770856169462204\n",
      "154 77.23875957200653 0.14492658487955729 0.13919684716860453 0.182625918674469 0.17552075517177582\n",
      "155 77.20428176000132 0.14529817235310874 0.13955450507799785 0.18231427803039552 0.17532564741373063\n",
      "156 76.95951988300658 0.14481646578470866 0.1390898466984431 0.1852074619293213 0.17814978778362275\n",
      "157 76.95196908799699 0.14458006240844729 0.13886486775875093 0.1834622857093811 0.17649916446208955\n",
      "158 79.28912559500895 0.14540124397277832 0.1396479424635569 0.183089715385437 0.17612147653102875\n",
      "159 76.97344139500638 0.1446970291519165 0.13897277030944824 0.1911292875289917 0.1843492334485054\n",
      "160 76.96474263098207 0.14471038890838622 0.1389920760790507 0.18419877815246583 0.17720547819137572\n",
      "161 76.98843030599528 0.14477548628489176 0.13904420571327208 0.18056821060180664 0.17352519154548646\n",
      "162 78.56156817998271 0.1450592919031779 0.13933516217867534 0.1845298186302185 0.17757945358753205\n",
      "163 77.00297615100862 0.143358783009847 0.1376922526915868 0.17806021575927736 0.17110630452632905\n",
      "164 76.98302713499288 0.14238206654866536 0.13677153792381286 0.17988786907196047 0.17295672643184662\n",
      "165 78.92574434299604 0.14327667467753094 0.13762764959335327 0.18712268161773682 0.18045318448543549\n",
      "166 77.85034984900267 0.14268008604685464 0.13705198506514232 0.17979547300338744 0.17281902927160264\n",
      "167 76.98914115098887 0.14243100697835287 0.13681060171127318 0.18206265783309936 0.17510198360681534\n",
      "168 77.27051153802313 0.14371022118886312 0.13803391726811726 0.17965035972595217 0.17273590677976608\n",
      "169 80.79952187099843 0.14339739453633626 0.1377201678276062 0.17960975370407103 0.17273292177915572\n",
      "170 76.9909345380147 0.14377122418721516 0.13808309365908306 0.1797194706916809 0.17279217374324798\n",
      "171 76.99125887401169 0.14341678184509277 0.1377478635629018 0.18014446725845337 0.17314558732509613\n",
      "172 76.98961675600731 0.14380855973561604 0.13811319812138875 0.18203375787734985 0.17499724090099333\n",
      "173 85.22525230899919 0.14296827121734618 0.13731200828552245 0.1788049849510193 0.17199169152975083\n",
      "174 93.10745029599639 0.14315851521809894 0.13749404978752136 0.18202778320312502 0.17495001804828644\n",
      "175 76.97833645599894 0.14328721857706705 0.13761513096491496 0.18318734893798827 0.17616035729646684\n",
      "176 76.97896017698804 0.14405128387451172 0.13834778591791788 0.18236445236206053 0.17533929264545442\n",
      "177 77.88091425900348 0.14322822244008382 0.13756692298253378 0.1813499744415283 0.17448338222503662\n",
      "178 77.03117272499367 0.1435410810470581 0.1378624326546987 0.18536803636550903 0.1782998925447464\n",
      "179 77.020111343998 0.143235086479187 0.13756234922409058 0.17873156776428223 0.17184914577007293\n",
      "180 77.06352046199027 0.1440590687688192 0.13835558522542318 0.18268812303543092 0.17564236897230148\n",
      "181 77.01409780999529 0.14311962582906088 0.13746200725237528 0.18081749963760377 0.17384937328100206\n",
      "182 77.14989454799797 0.14280461566925048 0.1371580956141154 0.1814467330932617 0.17460139977931977\n",
      "183 77.00207171600778 0.14243017411549885 0.1368085390408834 0.17808071546554566 0.17115673983097077\n",
      "184 77.05976643701433 0.14137385450998943 0.1357819585164388 0.17945986251831053 0.17250462859869004\n",
      "185 76.9725359499862 0.14272845072428386 0.13707586375872294 0.18220199069976806 0.1751439652442932\n",
      "186 80.41283907101024 0.14267782693227132 0.13702631417910258 0.17775865440368652 0.17087478786706925\n",
      "187 77.2966335680103 0.14245224100748696 0.13683002174695333 0.17812579736709594 0.17121500062942505\n",
      "188 77.04516685099225 0.14235225289662679 0.1367207939147949 0.1800088496208191 0.17303677093982697\n",
      "189 78.66626358998474 0.14250944550832112 0.1368665269056956 0.17992696323394775 0.17300057488679885\n",
      "190 79.39272309400258 0.14271162044525149 0.13706432835261026 0.17779450960159301 0.1709484691619873\n",
      "191 76.99097016599262 0.14187118227640788 0.1362531780163447 0.17640299282073973 0.16948066782951354\n",
      "192 91.05109641200397 0.1425193457921346 0.13687684257825217 0.18123274908065795 0.17414058667421342\n",
      "193 79.64073960197857 0.1420791167195638 0.13646288664340972 0.18437171096801758 0.17750672394037248\n",
      "194 76.95443028598675 0.1421876421737671 0.13655228599707286 0.18548395137786866 0.17863294035196303\n",
      "195 76.98535946197808 0.14231237197875976 0.13666987609863282 0.17813819637298584 0.17109468960762023\n",
      "196 76.97704453099868 0.14212577566782633 0.13649658603668213 0.1799770841598511 0.17309414422512054\n",
      "197 76.97488956298912 0.14226836681365967 0.1366236375808716 0.17842490577697753 0.171506108045578\n",
      "198 76.9500023930159 0.14252056184132894 0.13687504636446635 0.17940814323425294 0.17244589126110077\n",
      "199 77.08388883600128 0.1420592927424113 0.1364372239748637 0.1844655333518982 0.1774587241411209\n",
      "200 77.04316293200827 0.1341586438369751 0.1289571128845215 0.1719879370689392 0.16536145573854447\n",
      "201 77.0590647229983 0.13335243311564127 0.1281818073431651 0.1736114408493042 0.16697248667478562\n",
      "202 77.05142963799881 0.13308725087483725 0.12792072547276814 0.17201782283782957 0.1653730571269989\n",
      "203 77.0842423850263 0.13298835676829018 0.1278212966521581 0.17338857746124267 0.16678993391990662\n",
      "204 77.05514520700672 0.13238201992034912 0.1272401817480723 0.1719656343460083 0.16531010979413988\n",
      "205 77.05787997800508 0.13275696451822916 0.12758715764681497 0.17043390760421753 0.16369585341215134\n",
      "206 77.04119255000842 0.131958980178833 0.12682112748622895 0.1700850769996643 0.16345090734958648\n",
      "207 77.2495468170091 0.13149367397308348 0.1263796309709549 0.16825358400344848 0.16165241557359694\n",
      "208 83.26422487999662 0.13199905616760255 0.12685903052488962 0.17012773303985596 0.16360041302442552\n",
      "209 76.95042530301725 0.13161760917663573 0.12649588494300842 0.16923415784835816 0.16267104816436767\n",
      "210 89.5640666539839 0.13169743680318197 0.12656957127253216 0.1696170447349548 0.16299083685874938\n",
      "211 98.13192117898143 0.13208423933664956 0.12693324330647787 0.16943289566040037 0.16278713876008988\n",
      "212 87.84919897199143 0.13144546534220378 0.12632699117660523 0.168040029335022 0.1614801126718521\n",
      "213 77.0406060659734 0.13137057333628338 0.1262497066338857 0.1701274965286255 0.16345225787162782\n",
      "214 96.88255344799836 0.13113853262583414 0.12602739165623983 0.168149773979187 0.16158721029758452\n",
      "215 100.46135426699766 0.13055195466359457 0.12547720371087392 0.16661678113937378 0.16005821639299392\n",
      "216 99.4816100540047 0.13078236602783205 0.12568930413722992 0.16865140027999878 0.16203420668840407\n",
      "217 77.10359502100619 0.1302141843541463 0.1251529559850693 0.16820286054611205 0.16157911258935928\n",
      "218 77.00782355901902 0.13059533688863117 0.12550303477446237 0.1676453429222107 0.1610578083395958\n",
      "219 94.28075357998023 0.13071095213572184 0.12560821719964346 0.16750612087249755 0.16091392892599105\n",
      "220 84.31838752000476 0.13088409440358478 0.1257655616680781 0.1678004955291748 0.16121996605396272\n",
      "221 92.37426077001146 0.130835401802063 0.12572767461140952 0.16833257627487183 0.161724009513855\n",
      "222 77.07323868700769 0.13063087973276774 0.1255316482146581 0.16770128841400148 0.1611532056927681\n",
      "223 95.40149625900085 0.13053067222595213 0.1254348107655843 0.1653262861251831 0.1588062515258789\n",
      "224 79.13400234899018 0.13064373147328695 0.12554176003932954 0.16775141878128053 0.16113967597484588\n",
      "225 77.1340768999944 0.13074196343739827 0.12563298087914784 0.16698928422927856 0.1605104433298111\n",
      "226 77.13847372902092 0.13029554613749186 0.12521085387865702 0.16614299898147583 0.1596142851114273\n",
      "227 77.11446157598402 0.13041686234792074 0.1253209890762965 0.16578653564453125 0.15923816335201263\n",
      "228 77.3967164719943 0.12996893951416016 0.12489305040041605 0.16706953554153442 0.16051292884349824\n",
      "229 76.95802290202118 0.13027684525807698 0.1251862114429474 0.16796052808761597 0.16137312293052675\n",
      "230 76.96140311399358 0.12989147305806478 0.12482131236394246 0.1672963165283203 0.16066931784152985\n",
      "231 78.05999742998392 0.1299971141688029 0.1249169405301412 0.16619049520492554 0.15961418879032135\n",
      "232 76.9993437770172 0.12993610891977947 0.12485759256680806 0.16904790859222413 0.1624102761745453\n",
      "233 77.09875496098539 0.1296552642695109 0.12459770693778992 0.16408502254486085 0.15758919191360474\n",
      "234 76.93311694901786 0.12985100413004558 0.1247808175086975 0.16395014200210573 0.15746485191583634\n",
      "235 77.46662485701381 0.12967097675323486 0.124604829621315 0.1684900549888611 0.16200181800127028\n",
      "236 79.0323462029919 0.13038234471638996 0.1252783266146978 0.166356538105011 0.1598092954158783\n",
      "237 77.10017379699275 0.1300705795542399 0.1249803280433019 0.16472596559524538 0.1582266137599945\n",
      "238 76.96462048598914 0.1296561556752523 0.12458793200651805 0.16510949115753173 0.15862829184532165\n",
      "239 77.9253650069877 0.12934714790344237 0.12429173097610474 0.16649196147918702 0.1599938845038414\n",
      "240 77.01015744000324 0.1299679210662842 0.12488021672566732 0.1665358828544617 0.16000642186403274\n",
      "241 83.20845250200364 0.1294832484436035 0.12442602949937184 0.1648542468070984 0.1582913428544998\n",
      "242 76.94653464099974 0.12921878213246663 0.12416592547893524 0.16339536275863648 0.1569104804992676\n",
      "243 78.84422693800298 0.129452015914917 0.12438456530570983 0.16501938991546633 0.15853471809625624\n",
      "244 81.62351973599289 0.12924248068491617 0.12418837944666544 0.16719809646606446 0.16053232234716416\n",
      "245 78.467890445987 0.1296018084081014 0.12452419272263845 0.16788603067398072 0.16129996180534362\n",
      "246 77.03415053599747 0.12986721926371256 0.12477439494132996 0.1660195736885071 0.1594284816980362\n",
      "247 76.97062003900646 0.1285498388671875 0.12353752914269765 0.16347857465744017 0.15699072158336638\n",
      "248 76.96182069298811 0.12860036814371745 0.12357983730634053 0.1660448007583618 0.15964378553628922\n",
      "249 76.96679037300055 0.12852243078867592 0.12350303328037263 0.16472315692901612 0.15820681834220887\n",
      "250 77.5935742520087 0.12919529694875082 0.12413142227331797 0.16747783946990966 0.16088148671388627\n",
      "251 76.96342531198752 0.12899686317443848 0.12394295083681742 0.1653045739173889 0.15888616186380386\n",
      "252 76.95779158800724 0.12855441883087157 0.1235297457853953 0.16654667024612427 0.16003476929664612\n",
      "253 76.97234776700498 0.12841974773406983 0.12339634375572205 0.1665497630119324 0.16005410557985306\n",
      "254 83.205614978011 0.12866156909942628 0.12362566639582316 0.16376163959503173 0.15726897758245467\n",
      "255 76.94948711901088 0.12866032254536947 0.12362728594938914 0.16534435863494873 0.15882923763990403\n",
      "256 76.9777225299913 0.12862583160400393 0.12358568330605825 0.1650803493499756 0.158529344022274\n",
      "257 78.0207711319963 0.12834241530100504 0.12331171425183614 0.16609985141754152 0.1595925347805023\n",
      "258 77.5857694610022 0.12895490633646647 0.1238909554084142 0.1658786227226257 0.1593856866955757\n",
      "259 77.19168706599157 0.12846314360300698 0.12343357036908467 0.16715581665039064 0.16059901732206344\n",
      "260 77.0184855319967 0.12864326489766437 0.12360435938835145 0.16472079448699953 0.1582285628914833\n",
      "261 77.10081955901114 0.1287103825124105 0.12366925334135691 0.16679592666625978 0.16027409160137177\n",
      "262 81.39269918901846 0.1289909561284383 0.12392886253992716 0.16411730728149415 0.1576028351187706\n",
      "263 77.49801020501764 0.1291598433303833 0.12408888379732767 0.1760631416320801 0.16908569532632828\n",
      "264 77.20402191500762 0.12921482752482097 0.12414580731391907 0.16539027948379517 0.1588794391155243\n",
      "265 77.04353399897809 0.1285957878748576 0.12354990656375885 0.16660383090972902 0.1600109080672264\n",
      "266 77.00360759097384 0.12913871660868326 0.12406302626132965 0.16569371461868285 0.1592174841761589\n",
      "267 80.59246319401427 0.12839632940928142 0.12336054482460022 0.16569388437271118 0.15915524166822434\n",
      "268 89.21040660201106 0.12833962228139242 0.12331564389864604 0.1623425636291504 0.15593682545423507\n",
      "269 77.01961514499271 0.12847750162760416 0.12343233884970348 0.1679414373397827 0.1614759018421173\n",
      "270 82.21047295900644 0.12842673948923747 0.12338233766555787 0.1654218123435974 0.15890578639507294\n",
      "271 77.00897399801761 0.12878562441507974 0.12372769978046418 0.1644044560432434 0.157919051527977\n",
      "272 77.02846089898958 0.12842291152954102 0.12338828412691752 0.1628169485092163 0.15634300881624222\n",
      "273 83.13969291100511 0.12774777884165447 0.12274535833199819 0.16369412412643433 0.15726787948608398\n",
      "274 77.75878302700585 0.12827392012278238 0.12323845221996307 0.16724921169281007 0.16072713297605515\n",
      "275 76.98981919701328 0.1282987385559082 0.12326526977221171 0.16155568094253542 0.15516305911540984\n",
      "276 77.11816732300213 0.1274935924275716 0.12249721670150757 0.16462148838043214 0.15817120057344436\n",
      "277 77.09463513299124 0.1278959405008952 0.12288093708356221 0.16366179118156432 0.1571967307329178\n",
      "278 77.11381159999291 0.12742673784891764 0.12243255062898 0.1622787036895752 0.15583338433504104\n",
      "279 77.15883753501112 0.127208939259847 0.12223188101450602 0.17026644191741944 0.1636083919405937\n",
      "280 79.86211155398632 0.12758286178588868 0.12257190731366475 0.16130410490036012 0.1548533090353012\n",
      "281 77.06182487099431 0.1270930787785848 0.12211636196772258 0.1621526337623596 0.15576121151447297\n",
      "282 77.16075650099083 0.12785985111236572 0.12283545893828074 0.16415986013412476 0.15770337146520613\n",
      "283 77.22915977501543 0.12748533561706543 0.12248708333174388 0.16275703344345094 0.15634800946712493\n",
      "284 76.98557345400332 0.12797055592854817 0.1229369390487671 0.16315970287322998 0.1566355705857277\n",
      "285 76.97117119200993 0.1276964283243815 0.12268918078740437 0.1637576464653015 0.15730181938409804\n",
      "286 76.92824062198633 0.12795518352508545 0.12292840815385182 0.16778629360198977 0.1613552759885788\n",
      "287 76.98155483001028 0.1278116877746582 0.12278953471978506 0.16299645738601684 0.15653755402565003\n",
      "288 76.97689023098792 0.1273201587041219 0.12232573245366414 0.16250844974517822 0.1560784779191017\n",
      "289 77.043603728991 0.12789062379201252 0.12286209425926209 0.16585286874771119 0.15925621157884598\n",
      "290 77.03890759599744 0.12734138921101887 0.12234356195131937 0.16264392223358154 0.1562288367152214\n",
      "291 79.91850249300478 0.12766142280578613 0.12263264090220133 0.16757584466934203 0.1609477053284645\n",
      "292 86.12274868797977 0.1277000974782308 0.1226760911544164 0.16239596004486084 0.15593000835180282\n",
      "293 89.02927850899869 0.12779454509735105 0.12276331158479055 0.16325464277267457 0.15683496683835985\n",
      "294 77.00473891300499 0.12761150334676105 0.12259017130533854 0.16186616067886353 0.15549553793668747\n",
      "295 77.04026946600061 0.12726148382822672 0.12225710059801738 0.1627738832473755 0.15639566791057585\n",
      "296 76.97135776700452 0.12730591016133624 0.12230404411951701 0.1648448052406311 0.15833384132385253\n",
      "297 76.96526221500244 0.12763453093210858 0.12261608221530915 0.1616766396522522 0.15529006469249726\n",
      "298 77.11945751702297 0.12684379753112793 0.12186850062211355 0.16353084955215452 0.15708132165670394\n",
      "299 76.98918139698799 0.12692174574534099 0.12193389975229899 0.16246008052825928 0.15608101016283035\n",
      "300 76.96838000000571 0.12248652675628662 0.1177750781138738 0.15942859687805175 0.1532098754644394\n",
      "301 77.0139456379984 0.12225781416575114 0.11755389107863108 0.1585650683403015 0.15231689685583114\n",
      "302 77.03047299801256 0.12197803447723388 0.11728422836462657 0.15862161855697632 0.15234711015224456\n",
      "303 76.99438838200876 0.1217718744468689 0.11708212529023489 0.1589648756980896 0.15264904016256334\n",
      "304 76.9747626669996 0.12145562272389729 0.116784281762441 0.15899692306518554 0.15274785363674165\n",
      "305 77.01173373599886 0.12142844357808431 0.11675399281183879 0.1594620952606201 0.15313391011953353\n",
      "306 77.10424375900766 0.12140672836303712 0.11673148844242096 0.15874855608940125 0.15244771069288254\n",
      "307 76.96465259700199 0.12107697321573893 0.11642925554116566 0.15750416584014892 0.15126466929912566\n",
      "308 76.97405346002779 0.12093685395558676 0.11628125829696655 0.16219891443252563 0.1557494568824768\n",
      "309 77.08236564099207 0.12110948780059813 0.1164421812057495 0.15749294748306275 0.15126772445440292\n",
      "310 77.10325808401103 0.12119982957204183 0.11652973444461823 0.157848149394989 0.15162274235486983\n",
      "311 77.06692868698156 0.12073576885223389 0.11608954879442851 0.15690432062149048 0.15068855571746825\n",
      "312 77.09188120200997 0.12083099343617758 0.11617575756708781 0.15796262474060058 0.15170545029640198\n",
      "313 77.10139656698448 0.12113792329152424 0.11646526648998261 0.15851995429992677 0.15222670185565948\n",
      "314 77.13142044498818 0.12053072176615398 0.11588876022497813 0.15770004014968872 0.1514072617292404\n",
      "315 77.00681502601947 0.12071270220438639 0.11606220804055532 0.1570644371032715 0.15084812927246094\n",
      "316 77.20855439198203 0.12075899110158284 0.1160990335226059 0.15648940777778625 0.1502561799287796\n",
      "317 77.0068723190052 0.12079479732513428 0.11612439001401266 0.15732001686096192 0.15103930062055587\n",
      "318 77.22407880501123 0.12066702606201171 0.11600758566856384 0.15810554523468018 0.1518137326836586\n",
      "319 77.25458689199877 0.12054148540496826 0.1158863807439804 0.1576847831249237 0.15139409893751143\n",
      "320 77.02779439199367 0.12061322919209798 0.11596002915700276 0.15729714469909667 0.1510286452770233\n",
      "321 78.90254994799034 0.12040377384185792 0.11574973793824514 0.15797735919952394 0.1517216433286667\n",
      "322 77.04755175099126 0.12056499404907225 0.11590663101673127 0.1578316240310669 0.15155711966753005\n",
      "323 77.0022023920028 0.120348716990153 0.11570493528048198 0.1574283884048462 0.15114152717590332\n",
      "324 77.26642227501725 0.12033524909973145 0.11568203350702921 0.15707479104995728 0.15079989153146744\n",
      "325 78.29888036701595 0.12021643145243326 0.11556828902562459 0.15654110631942747 0.1503411421775818\n",
      "326 78.38054687299882 0.12020320236206054 0.11555021088918051 0.15618319272994996 0.1499633116722107\n",
      "327 77.05355562301702 0.12018236848195393 0.11553989840348562 0.15935826921463012 0.15302971667051316\n",
      "328 77.04913659399608 0.12014057139078775 0.11549611185391744 0.1578687493801117 0.15160699832439423\n"
     ]
    }
   ],
   "source": [
    "#VIC this needs a couple of touch ups, as at the bottom of: https://github.com/zongyi-li/fourier_neural_operator/blob/master/fourier_2d_time.py\n",
    "# also, i made very minor changes to the original code, to make the logic clearer\n",
    "\n",
    "if torch.cuda.is_available() :\n",
    "  model = Net2d(modes, width, T_in).cuda()\n",
    "  device  = torch.device('cuda')\n",
    "  print(\" Utilizing CUDA\")  \n",
    "else :\n",
    "  model = Net2d(modes, width, T_in)\n",
    "  device  = torch.device('cpu')\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "print(torch.cuda.is_available())    \n",
    "\n",
    "\n",
    "print(model.count_params())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
    "\n",
    "\n",
    "# myloss = LpLoss(size_average=False)\n",
    "\n",
    "#VIC these are not needed anymore\n",
    "# gridx = gridx.to(device)\n",
    "# gridy = gridy.to(device)\n",
    "\n",
    "\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "step = 1\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    t1 = default_timer()\n",
    "    train_l2_step = 0\n",
    "    train_l2_full = 0\n",
    "    for xx, yy in train_loader:\n",
    "        loss = 0\n",
    "        xx = xx.to(device)\n",
    "        yy = yy.to(device)\n",
    "\n",
    "        for t in range(0, T_out, step):\n",
    "            y = yy[..., t:t + step]\n",
    "            im = model(xx)\n",
    "            loss += myloss(im.reshape(batch_size, -1), y.reshape(batch_size, -1))\n",
    "\n",
    "            if t == 0:\n",
    "                pred = im\n",
    "            else:\n",
    "                pred = torch.cat((pred, im), -1)\n",
    "\n",
    "            xx = torch.cat((xx[..., step:], im), dim=-1)\n",
    "\n",
    "        train_l2_step += loss.item()\n",
    "        l2_full = myloss(pred.reshape(batch_size, -1), yy.reshape(batch_size, -1))\n",
    "        train_l2_full += l2_full.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test_l2_step = 0\n",
    "    test_l2_full = 0\n",
    "    with torch.no_grad():\n",
    "        for xx, yy in test_loader:\n",
    "            loss = 0\n",
    "            xx = xx.to(device)\n",
    "            yy = yy.to(device)\n",
    "\n",
    "            for t in range(0, T_out, step):\n",
    "                y = yy[..., t:t + step]\n",
    "                im = model(xx)\n",
    "                loss += myloss(im.reshape(batch_size, -1), y.reshape(batch_size, -1))\n",
    "\n",
    "                if t == 0:\n",
    "                    pred = im\n",
    "                else:\n",
    "                    pred = torch.cat((pred, im), -1)\n",
    "\n",
    "                xx = torch.cat((xx[..., step:], im), dim=-1)\n",
    "\n",
    "            test_l2_step += loss.item()\n",
    "            test_l2_full += myloss(pred.reshape(batch_size, -1), yy.reshape(batch_size, -1)).item()\n",
    "\n",
    "    t2 = default_timer()\n",
    "    scheduler.step()\n",
    "    print(ep, t2 - t1, train_l2_step / ntrain / (T_out / step), train_l2_full / ntrain, test_l2_step / ntest / (T_out / step),\n",
    "          test_l2_full / ntest)\n",
    "    \n",
    "# add loss to name, with 4 decimals    \n",
    "final_training_loss = '{:.4f}'.format(test_l2_full / ntest)\n",
    "final_training_loss = final_training_loss.replace('.', '@')\n",
    "\n",
    "model_name = model_name+'_loss'+final_training_loss   \n",
    "model_full_path = model_path+model_name\n",
    "\n",
    "torch.save(model, model_full_path)\n",
    "\n",
    "#path_train_err = model_path+'results/'+model_name+'_train.txt'\n",
    "#path_test_err = model_path+'results/'+model_name+'_test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yphgOwKoKsHL"
   },
   "source": [
    "To avoid automatic runtime disconnection for inactivity [90 minutes]:\n",
    "\n",
    "_press ctlr+shift+i\n",
    "\n",
    "_then go to the console and type:\n",
    "\n",
    "```\n",
    "function ClickConnect(){\n",
    "  console.log(\"Connnect Clicked - Start\"); \n",
    "  document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
    "  console.log(\"Connnect Clicked - End\"); \n",
    "};\n",
    "setInterval(ClickConnect, 60000)\n",
    "```\n",
    "\n",
    "\n",
    "This js code will click the \"Connect\" button [top right of Colab notebook] every minute...\n",
    "Maximum runtime life remains 12 hours\n",
    "\n",
    "Found here: https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting?page=1&tab=votes#tab-top"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "history_visible": true,
   "name": "iwe_m2d_win_train(with other dataset).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
